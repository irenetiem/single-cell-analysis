{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Generalized linear models (GLMs) of neural responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.5em;background-color:#f1f1f1;border:1px solid black;width:95%'>\n",
    "\n",
    "Lesson time: 60 m   \n",
    "Contributors: Arne Mayer, Davide Spalla\n",
    "\n",
    "---\n",
    "### In this lesson you will learn:\n",
    "- What GLMs are\n",
    "- How they can be used to model the response to single neurons to different stimuli\n",
    "- What is the difference between a purely linear, a gaussian and a poisson response model\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    " \n",
    "In this lesson you will learn how to model the response of a neuron as a function of an external stimulus using [Generalized linear models (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model).\n",
    "\n",
    "These concept will be particularly useful in the study of early sensory processes, where the activity of single neurons can be well carachterized as as an **input-output function** â€“ taking a sensory stimulus (such as an pattern of photons hitting our retina, or the mechanical vibrations that we pereive as sounds) and producting a pattern of actvity.   \n",
    "\n",
    "This phenomenon can be studied with many different tools: tuning curves (ref), ... to name a few. \n",
    "In this lesson we will focus on GLM, a class of simple but powerful models that can be use to describe how a the firing rate of a neuron depends on the value of sensory stimuli.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulus-response functions\n",
    "---\n",
    "A major challenge with characterizing neural responses to sensory stimuli is that the possible space of inputs is very large, possibly infinite. An example from the auditory field might help to illustrate the problem. Imagine that you have a piano with 88 keys and you want to understand how a neuron in the auditory cortex is processing sounds (here: the tones that are generated when pressing one or more piano keys). All sensory neurons are more or less nonlinear. Thus the neural response to the combination of keys C1 and E1 is not the same as the sum of C1 or E1 when presented alone. As a consequence, probing the neuron with all possible key combinations would require $2^{88} - 1$ \"stimuli\". Even if we fix the duration of each stimulus to 1 second and use the same sound level this would exceed the lifetime of any species used to study these principles in the laboratory. Thus, we have to do something smarter.\n",
    "\n",
    "\n",
    "**Generalized linear models (GLMs)**\n",
    "\n",
    "\n",
    "An alternative approach is to use a statistical model that seeks to characterize response to complex stimuli. \n",
    "This allows us to restrict the shape that the neural response can take to a much smaller hypothesis space, using some assumption on the kind of function that the neuron implements\n",
    "\n",
    "GLMs are a powerful and popular set of such statistical models, that hypothesizes that the neural responses is a result of two stages: a linear filter of the input and a static non-linearity.\n",
    "\n",
    "**1. Linear Stage**\n",
    "\n",
    "The **linear stage** consists of one or more **linear filters** represented by the vectors $\\mathbf{k}_1, \\mathbf{k}_2, ...$ that describes how a neuron is integrating stimulus features (defined below). In this leson we will focus on models with a single filter $\\mathbf{k}$. The linear filters are linked to the concept of a **receptive field (RF)** and indeed both terms are used interchangeably. Broadly speaking, the RF is a portion of **sensory space** that can elicit neuronal responses when stimulated. For example, a neuron in the visual cortex that elicits an action potential when an object appears in the upper left visual field is said to have a RF at that location. For a visual neuron, the sensory space is typically 2-dimensional (e.g., a x/y positions of pixels on a display used in laboratory settings) or 3-dimensional (e.g., pixel x/y positions that change over time). The precise geometry of the sensory space does often not matter. Instead, we treat every \"element\" (e.g., a pixel of an image) of this space as a single dimension in a usually high-dimensional \"feature\" space (see above image) and write the stimulus as a vector \n",
    "\n",
    "$$\n",
    "\\mathbf{s} = (s_1, s_2, s_3, ..., s_D)^T.\n",
    "$$\n",
    "\n",
    "The output of the linear stage is simply the dot product of the linear filter and the stimulus:\n",
    "\n",
    "$$\n",
    "x = \\mathbf{k}^T \\mathbf{s} = \\sum_{i=1}^D k_i s_i.\n",
    "$$\n",
    "\n",
    "Thus, the entries in the vector $\\mathbf{k}$ can be interpreted as the weights that a neuron gives to different stimulus features. For the visual neuron mentioned above, the weights would be non-zero for a pixel values in the upper left visual field, and zero otherwise. It is exactly this **simple interpretation of model parameters in the original stimulus space** that makes GLMs so appealing and is one or the main reasons why it is extremely popular in neural coding. \n",
    "\n",
    "**2. Nonlinear Stage**\n",
    "\n",
    "The nonlinear stage that transforms the linearly filtered stimulus $x$ into a spike rate using a static, memoryless nonlinearity $f$. The output of the nonlinear stage for a single filter is given by $f(\\mathbf{k}^T\\mathbf{s})$. For dynamically fluctuating stimuli, the time-varying output of the LN model can simply be described by $f(\\mathbf{k}^T\\mathbf{s}_t)$ where $\\mathbf{s}_t$ is the stimulus at discrete time step $t$.\n",
    "\n",
    "The type of non-linearity uses in the non-linear stage defines the different kind of GLM\n",
    "In this lesson, we will discuss the **linear-Gaussian model** and the **linear-nonlinear Poisson model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-Gaussian models\n",
    "---\n",
    "\n",
    "In the simplest case the response is assumed to be modeled directly by the output of a single filter, possibly with a constant offset response:\n",
    "\n",
    "$$\n",
    "    r_t = \\mathbf{k}^T \\mathbf{s}_t + r_0 + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "where response variability (which inevatibly arises in neural data) is Gaussian-distributed with constant variance $\\sigma^2$ around the filter output. The constant offset term $r_0$ can be conveniently absorbed into the RF vector $\\mathbf{k}$ by setting an additional dimension in the stimulus vector $\\mathbf{s}_t$ to 1 at all times, so that the offset becomes the coefficient associated with this added dimension. Thus, we will typically omit explicit reference to (and notation of) the offset term.\n",
    "\n",
    "Given a stimulus and a measured response, estimated filter weights $\\hat{\\mathbf{k}}$ can be obtained by minimizing the squared difference between the model output and the measured data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{k}} = \\underset{k}{\\mathrm{argmin}} \\sum_{t=1}^{N} || r_t - \\mathbf{k}^T \\mathbf{s}_t||^2 = (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{S}^T\\mathbf{r}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{S}$ is the stimulus design matrix formed by collecting the stimulus vectors as rows, $\\mathbf{S} = (\\mathbf{s}_1, \\mathbf{s}_2, ..., \\mathbf{s}_N)^T$ and $\\mathbf{r}$ is a column vector of corresponding measured responses. The matrix product $\\mathbf{S}^T \\mathbf{r}$ gives the sum of all stimuli that evoked spikes (with stimuli evoking multiple spikes repeated for each spike in the bin); if divided by the total number of spikes this would be the spike-triggered average (STA) stimulus. The term $\\mathbf{S}^T \\mathbf{S}$ is the stimulus auto-correlation matrix; pre-multiplying by its inverse removes any structure in the STA that might arise from correlations between different stimulus inputs, leaving an estimate of the RF filter. \n",
    "\n",
    "More generally, the above equations corresponds to the maximum likelihood estimator (MLE) for a model in which response variability is Gaussian-distributed with constant variance around the filter output $x_t$ (for details see the review paper mentioned above).\n",
    "\n",
    "Before we start with the linear model, we define a number of helper functions to that are generally useful for data generation throughout the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_receptive_field(size=(15, 15),\n",
    "                           mu=(8, 8),\n",
    "                           sigma=(4, 4),\n",
    "                           angle=45,\n",
    "                           frequency=.085,\n",
    "                           phase=0.):\n",
    "    \"\"\"2D Gabor-like receptive field\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size: tuple\n",
    "      (width, height) of the 2D RF\n",
    "    mu: tuple\n",
    "      (x, y) center coordinates of the Gaussian envelope\n",
    "    sigma: tuple\n",
    "      standard deviation of the Gaussian envelope\n",
    "    angle: float\n",
    "      Angle of the grating (in degrees)\n",
    "    frequency: float\n",
    "      Spatial grating frequency\n",
    "    phase: float\n",
    "      Spatial phase (in degrees)\n",
    "    \"\"\"\n",
    "\n",
    "    xx, yy = np.meshgrid(1. + np.arange(size[0]),\n",
    "                         1. + np.arange(size[1]))\n",
    "\n",
    "    # Gaussian envelope\n",
    "    G = np.exp(- np.power(xx - mu[0], 2) / (2. * sigma[0])\n",
    "               - np.power(yy - mu[1], 2) / (2. * sigma[1]))\n",
    "\n",
    "    # spatial modulation\n",
    "    phi = np.deg2rad(angle)\n",
    "    xxr = xx * np.cos(phi)\n",
    "    yyr = yy * np.sin(phi)\n",
    "    xyr = (xxr + yyr) * 2. * np.pi * 2. * frequency\n",
    "    S = np.cos(xyr + phase)\n",
    "\n",
    "    K = G * S\n",
    "    K /= np.amax(np.abs(K))\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def create_gaussian_stimuli(n_bins, n_dim,\n",
    "                            std_dev=1.,\n",
    "                            append_ones=True):\n",
    "    \"\"\"Gausssian white noise stimulus matrix\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    n_bins: int\n",
    "        Number of time steps\n",
    "    n_dim: int\n",
    "        RF dimensionality (= stimulus space dimensionality)\n",
    "    std_dev: float\n",
    "        Standard deviation of the Gaussian white noise\n",
    "    append_ones: bool\n",
    "        Append a column of ones to the stimulus matrix\n",
    "    \"\"\"\n",
    "\n",
    "    S = std_dev * np.random.randn(n_bins, n_dim)\n",
    "\n",
    "    if append_ones:\n",
    "        # append a row with ones for fitting the offset term\n",
    "        S = np.hstack((S, np.ones((n_bins, 1))))\n",
    "\n",
    "    return S\n",
    "\n",
    "def f_identity(x):\n",
    "    # identity function used in a truly linear model\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def f_threshold_quadratic(x):\n",
    "    # threshold-quadratic nonlinearity\n",
    "\n",
    "    y = np.copy(x)\n",
    "    y[y < 0] = 0\n",
    "\n",
    "    return y**2\n",
    "\n",
    "\n",
    "def f_quadratic(x):\n",
    "    # fully quadratic nonliearity\n",
    "\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def generate_data_linear(rf_size=(15, 15),\n",
    "                         f_nonlin=f_identity,\n",
    "                         duration=10.,\n",
    "                         dt=.1,\n",
    "                         offset=2.,\n",
    "                         noise_variance=4):\n",
    "    \"\"\"create model, stimulus set and and simulate neural response\n",
    "    \n",
    "    The model consists of the two stages:\n",
    "    1. A 2-dimensional RF that is used to filter 2D Gaussian white noise stimuli\n",
    "    2. A threshold-linear nonlinearity (as neural firing rates cannot be negative)\n",
    "\n",
    "    Note: by using a large offset term (r_0) the model can be made linear as the product k x s will be \n",
    "    positive. However, for the chosen stimulus class (Gaussian white noise) the linear estimator provides \n",
    "    an unbiased estimate even in the presence of a nonlinearity (Bussgang Res. Lab. Elec. (1952), Paninski \n",
    "    Network (2003)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    duration: float\n",
    "        Length of the data sequence in seconds\n",
    "    f_nonlin: function-like\n",
    "        The (potentially nonlinear) function that transforms k x s into a neural response (default: identity)\n",
    "    dt: float\n",
    "        Bin width (= time resolution) in seconds\n",
    "    offset: float\n",
    "        Offset term (see \"r_0\" in the description above)\n",
    "    noise_variance: float\n",
    "        Response noise variance\n",
    "    \"\"\"\n",
    "\n",
    "    assert duration > 0 and dt > 0 and noise_variance >= 0\n",
    "\n",
    "    # get number of non-overlapping time bins\n",
    "    n_bins = round(duration / float(dt))\n",
    "\n",
    "    # Gabor-like receptive field\n",
    "    K = create_receptive_field(size=rf_size)  # 2D RF -> matrix\n",
    "\n",
    "    # append entry to \"true\" RF (ravel() vectorizes the 2D RF matrix into a 1D vector)\n",
    "    k_with_offset = np.append(K.ravel(), offset)\n",
    "\n",
    "    # generate Gaussian stimuli\n",
    "    n_dim = K.size  # dimensionality of \"vectorized\" RF (= dim. of stimuli); the same as K.shape[0]*K.shape[1]\n",
    "    S = create_gaussian_stimuli(n_bins, n_dim,\n",
    "                                append_ones=True)\n",
    "\n",
    "    # 1. linear stage\n",
    "    ks = np.dot(k_with_offset, S.T)\n",
    "\n",
    "    # 2. nonlinear stage (for a truly linear model: f -> identity function)\n",
    "    rate = f_nonlin(ks)\n",
    "\n",
    "    # add Gaussian noise centered around the \"true\" rate for each bin\n",
    "    rate = rate + np.sqrt(noise_variance) * np.random.randn(n_bins)\n",
    "\n",
    "    return K, S, ks, rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot model parameters together with the generated neural response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_identity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7bc282bc430e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf_nonlin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_identity\u001b[0m  \u001b[0;31m# for a linear model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m K_true, S, ks, rate = generate_data_linear(f_nonlin=f_nonlin,\n\u001b[1;32m      6\u001b[0m                                            \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_identity' is not defined"
     ]
    }
   ],
   "source": [
    "# simulate data\n",
    "duration = 10.\n",
    "dt = .1\n",
    "f_nonlin = f_identity  # for a linear model\n",
    "K_true, S, ks, rate = generate_data_linear(f_nonlin=f_nonlin,\n",
    "                                           duration=duration,\n",
    "                                           dt=dt)\n",
    "\n",
    "# show example stimulus, receptive field, and static nonlinearity\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title(r'Example Gaussian white noise stimulus $\\mathbf{s}$')\n",
    "ax.imshow(S[0, :-1].reshape(K_true.shape))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(r'Receptive field $\\mathbf{k}$')\n",
    "ax.imshow(K_true, vmin=-1, vmax=1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title('Linear transform (+ Gaussian noise)')\n",
    "xx = np.linspace(ks.min(), ks.max(), 100)\n",
    "ax.plot(xx, f_nonlin(xx), 'r-')\n",
    "ax.scatter(ks, rate, s=10, c=[3*[.1]])\n",
    "ax.set_xlabel(r'$\\mathbf{k}^T \\mathbf{s}$')\n",
    "ax.set_ylabel(r'Rate $r$')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plot spike rate together with spikes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "n_bins = len(rate)\n",
    "t = (.5 + np.arange(n_bins)) * dt # show bin centers\n",
    "ax.plot(t, ks, '-',\n",
    "        color='tab:blue',\n",
    "       label=r'$\\mathbf{k}^T\\mathbf{s}$')\n",
    "ax.plot(t, rate, '-',\n",
    "       color='tab:orange',\n",
    "       label=r'Rate $r$')\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel(r'Rate $r$')\n",
    "ax.legend(loc=(.05, 1.05),\n",
    "         fontsize=8).get_frame().set_visible(0)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-nonlinear Poisson models\n",
    "---\n",
    "\n",
    "For spike-train responses, a natural first assumption is that spike times are influenced only by the stimulus, and are otherwise entirely independent of one another. This assumption requires that the distribution of spike times be governed by a **Poisson (point) process conditioned on the stimulus**, defined by an **instantaneous rate function**\n",
    "\n",
    "$$\n",
    "    \\lambda_t = f(\\mathbf{k}^T \\mathbf{s}_t)\n",
    "$$ \n",
    "\n",
    "(often also called intensity function). In turn, this means that the distributions of counts within response time bins of size $\\Delta$ must follow a Poisson distribution\n",
    "\n",
    "\\begin{equation}\n",
    "  P(r_t | \\mathbf{s}_t, \\mathbf{k}) = \\frac{(\\lambda_t\\Delta)^{r_t}}{r_t!} e^{-\\lambda_t\\Delta}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda_t\\Delta$ is the expected number of spikes in a small unit of time $\\Delta$ and $\\lambda_t$ is the rate (or intensity) of the Poisson process. As for the linear-Gaussian model, a constant offset term $r_0$ can be conveniently absorbed into the RF vector $\\mathbf{k}$ by setting an additional dimension in the stimulus vector $\\mathbf{s}_t$ to 1 at all times.\n",
    "\n",
    "If $f$ is assumed to be monotonic and fixed (rather than being defined by parameters that must be fit along with the RF) then the above equation describes an instance of a **generalized linear model (GLM)** (Nelder and Wedderburn, 1972), a widely-studied class of regression models. Many common choices of $f$ result in a likelihood which is a concave function (Paninski, 2004), guaranteeing the existence of a **single optimum** that can easily be found by standard optimization techniques such as gradient ascent or Newton's method. The GLM formulation can also be extended to non-Poisson processes, by including probabilistic interactions between spikes in different bins that may be often reminiscent of cellular biophysical processes. However, here we will focus on a Poisson GLM without spike bin interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inhomogeneous_poisson_spikes(lamda, dt):\n",
    "\n",
    "    n_bins = lamda.shape[0]\n",
    "    bins = np.arange(n_bins+1)*dt\n",
    "\n",
    "    # generate Poisson distributed numbers for all bins with the max. intensity (lamda_max)\n",
    "    lamda_max = np.max(lamda)\n",
    "    poisson_numbers = np.random.poisson(lamda_max, size=n_bins)\n",
    "\n",
    "    # throw away numbers depending on the actual intensity (\"thinning\")\n",
    "    spike_times = []\n",
    "    prob = lamda / lamda_max\n",
    "    for i in range(n_bins):\n",
    "        \n",
    "        # number of spikes to keep in this bin\n",
    "        n = np.sum(np.random.rand(poisson_numbers[i]) < prob[i])\n",
    "        n_s = int(round(n * dt))\n",
    "\n",
    "        # generate random spike times in this bin\n",
    "        ts = bins[i] + np.random.rand(n_s)*dt\n",
    "\n",
    "        spike_times.extend(ts)\n",
    "\n",
    "    return np.asarray(spike_times)\n",
    "\n",
    "\n",
    "def generate_data_poisson(rf_size=(15, 15),\n",
    "                          duration=10.,\n",
    "                          dt=.1,  # delta in equation\n",
    "                          offset=0.,\n",
    "                          spike_rate=5.  # average spike rate\n",
    "                          ):\n",
    "\n",
    "    assert duration > 0 and dt > 0\n",
    "\n",
    "    # get number of non-overlapping time bins\n",
    "    n_bins = round(duration / float(dt))\n",
    "\n",
    "    # Gabor-like receptive field\n",
    "    K = create_receptive_field(size=rf_size)  # 2D RF -> matrix\n",
    "\n",
    "    # generate Gaussian stimuli\n",
    "    n_dim = K.size  # dimensionality of \"vectorized\" RF (= dim. of stimuli); the same as K.shape[0]*K.shape[1]\n",
    "    S = create_gaussian_stimuli(n_bins, n_dim,\n",
    "                                std_dev=.75,  # gives a more realistic spike rate (exp() nonlinearity can produce very large values)\n",
    "                                append_ones=True)\n",
    "\n",
    "    # append entry to \"true\" RF (ravel() vectorizes the 2D RF matrix)\n",
    "    k_with_offset = np.append(K.ravel(), offset)\n",
    "\n",
    "    # 1. linear stage\n",
    "    ks = np.dot(k_with_offset, S.T)\n",
    "\n",
    "    # 2. nonlinear stage\n",
    "    lamda = np.exp(ks)\n",
    "\n",
    "    # lamda * dt is the number of spikes in the different bins (but keep in mind that the Poisson process\n",
    "    # is a stochastic process so the actual number will differ for every draw). Thus, the sum of the product \n",
    "    # across all bins gives the expected number of spikes for the whole draw.\n",
    "    expected_rate = np.sum(lamda*dt) / duration\n",
    "    lamda *= (spike_rate / expected_rate)\n",
    "\n",
    "    # generate spike times using an inhomogeneous Poisson process\n",
    "    spike_times = generate_inhomogeneous_poisson_spikes(lamda, dt)\n",
    "\n",
    "    # compute spike counts in the different time bins\n",
    "    spike_counts = np.histogram(spike_times,\n",
    "                                bins=np.arange(n_bins+1)*dt)[0]\n",
    "\n",
    "    print(\"average spike rate: %0.2f spikes per second\" % (len(spike_times) / duration))\n",
    "\n",
    "    return K, S, ks, lamda, spike_times, spike_counts\n",
    "\n",
    "\n",
    "# show model parameters\n",
    "duration = 10.\n",
    "dt = .1\n",
    "K_true, S, ks, lamda, spike_times, spike_counts = generate_data_poisson(duration=duration,\n",
    "                                                                        dt=dt)\n",
    "\n",
    "\n",
    "# show example stimulus, receptive field, and static nonlinearity\n",
    "fig, axes = subplots(nrows=1, ncols=3)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title(r'Example Gaussian white noise stimulus $\\mathbf{s}$')\n",
    "ax.imshow(S[0, :-1].reshape(K_true.shape))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(r'Receptive field $\\mathbf{k}$')\n",
    "ax.imshow(K_true, vmin=-1, vmax=1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title('Static nonlinearity (exponential)')\n",
    "ax.scatter(ks, lamda, s=10, c=[3*[.1]])\n",
    "ax.set_xlabel(r'$\\mathbf{k}^T \\mathbf{s}$')\n",
    "ax.set_ylabel(r'f($\\mathbf{k}^T \\mathbf{s})$')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show response\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True)\n",
    "\n",
    "n_bins = lamda.shape[0]\n",
    "t = np.arange(n_bins) * dt\n",
    "\n",
    "ax1.step(t, ks, where='post')\n",
    "ax1.set_ylabel(r'$\\mathbf{k}^T \\mathbf{s}$')\n",
    "\n",
    "ax2.step(t, lamda, where='post')\n",
    "ax2.set_ylabel(r'$\\lambda$')\n",
    "\n",
    "ax3.vlines(spike_times, 0, 1)\n",
    "ax3.set_ylabel('1 = spike')\n",
    "\n",
    "ax4.step(t, spike_counts, where='post')\n",
    "ax4.set_xlabel('Time (s)')\n",
    "ax4.set_ylabel('Spike count')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function and its derivative w.r.t $\\mathbf{k}$ for a LNP model with exponential nonlinearity $f(\\mathbf{k}^T \\mathbf{s}) \\equiv e^{\\mathbf{k}^T \\mathbf{s}}$ can be computed as follows: \n",
    "  \n",
    "Log-likelihood for a single observation (at time $t$):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\log P(r_t | \\lambda_t) = r_t\\log \\lambda_t + r_t\\log \\Delta - \\log r_t! - \\lambda_t \\Delta.\n",
    "\\end{equation}\n",
    "\n",
    "Thus the log-likelihood for the whole spike sequence is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\log P(R | \\lambda) = \\sum_t r_t\\log \\lambda_t + \\sum_t r_t\\log \\Delta - \\sum_t \\log r_t! - \\sum_t \\lambda_t \\Delta.\n",
    "\\end{equation}\n",
    "\n",
    "The derivative w.r.t to $\\mathbf{k}$ is\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial{\\log P}}{\\partial{\\mathbf{k}}} &= \\sum_t \\frac{\\partial{r_t \\log \\lambda_t}}{\\partial{\\mathbf{k}}} + \\sum_t \\frac{\\partial{r_t \\log \\Delta}}{\\partial{\\mathbf{k}}} - \\sum_t \\frac{\\partial{r_t \\log r_t!}}{\\partial{\\mathbf{k}}} - \\Delta \\sum_t \\frac{\\partial{\\lambda_t}}{\\partial{\\mathbf{k}}} \\\\\n",
    "    &= \\sum_t \\frac{\\partial{r_t \\log \\lambda_t}}{\\partial{\\mathbf{k}}} - \\Delta \\sum_t \\frac{\\partial{\\lambda_t}}{\\partial{\\mathbf{k}}}\n",
    "\\end{align}\n",
    "\n",
    "and with $\\frac{\\partial{\\lambda_t}}{\\partial{\\mathbf{k}}} = \\mathbf{s}\\lambda_t$\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\log P}}{\\partial{\\mathbf{k}}} = \\sum_t r_t \\mathbf{s}_t - \\Delta \\sum_t \\mathbf{s}_t \\lambda_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL DATA EXAMPLE \n",
    "- head direction cells? V1 gabor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.5em; background-color:#f1f1f1;border:1px solid black;width:95%'>\n",
    "\n",
    "### Key points \n",
    "\n",
    "- GLMs are statistical models of neural responses\n",
    "- They consist of a linear filter and a static non-linearity\n",
    "- GLMs receptive fields shows to which part of the stimulus space a neuron responds preferentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.5em; background-color:#f1f1f1;border:1px solid black;width:95%'>\n",
    "\n",
    "### References and resources\n",
    "\n",
    "**Books & papers**\n",
    "* A review on stimulus-response analysis: https://www.frontiersin.org/articles/10.3389/fnsys.2016.00109/full\n",
    "* A review on receptive-fields identification: https://www.annualreviews.org/doi/10.1146/annurev-neuro-062012-170253 \n",
    "\n",
    "**Websites & blogposts**\n",
    "\n",
    "\n",
    "**Software**\n",
    "* statsmodels offers an API for GLMs in python: https://www.statsmodels.org/stable/glm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "You can find the exercises for this lessons in [02-exercises.ipynb](02-GLMs/02-exercises.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec757111aa82fc412dab5a41ba1a33fdb6db5c8112df3ff06fec0dbff050b412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
